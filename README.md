# üìù Sentiment Analysis: BERT (RoBERTa-base) vs Naive Bayes

–í —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ —è –ø—Ä–æ–≤—ë–ª —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –∞–Ω–∞–ª–∏–∑—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞:

- üîπ **–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å** ‚Äî –ø—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è NLP.  
- üîπ **–î–æ–æ–±—É—á–µ–Ω–Ω—ã–π BERT (RoBERTa-base)** ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å, –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–º [–¥–∞—Ç–∞—Å–µ—Ç–µ](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).  

---

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

- **–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å** —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏, –Ω–æ —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∏ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.  
- **RoBERTa-base (fine-tuned)** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º.  

–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ **Hugging Face Hub**:  
üëâ [levshaone/model_sentiment](https://huggingface.co/levshaone/model_sentiment)

---

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

### üîπ –ü—Ä—è–º–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face Hub
model_name = "levshaone/model_sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
model.eval()

id2label = {0: "negative", 1: "positive"}

def predict(text: str):
    encoding = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**encoding)
        preds = torch.argmax(outputs.logits, dim=-1).item()
    return id2label[preds]

print(predict("I really love this movie!"))   # positive
print(predict("This film was terrible..."))   # negative
```
### üîπ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ Telegram-–±–æ—Ç–∞
–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face –µ—ë –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –ø—Ä–∏–º–µ–Ω—è—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö.  
–û–¥–∏–Ω –∏–∑ —É–¥–æ–±–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ ‚Äî –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ **Telegram-–±–æ—Ç–∞**, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.
–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏
```python
from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, MessageHandler, filters
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

MODEL_NAME = "levshaone/model_sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

labels = {
    0: "Negative",
    1: "Positive"
}

def analyze_text(text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=-1)
    label_id = torch.argmax(probs, dim=-1).item()
    label_name = labels[label_id]
    return f"Sentiment: {label_name}"

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text
    result = analyze_text(text)
    await update.message.reply_text(result)

if __name__ == "__main__":
    TOKEN = "YOUR_TELEGRAM_BOT_TOKEN"  # —Ç–æ–∫–µ–Ω –±–æ—Ç–∞
    app = ApplicationBuilder().token(TOKEN).build()

    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("Bot started...")
    app.run_polling()
```

## üì± –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã Telegram-–±–æ—Ç–∞

–ù–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –ø—Ä—è–º–æ –≤ —á–∞—Ç–µ:

![positive example](https://github.com/icelevsha/BERT_vs_Naive_Bayes/blob/main/images/negative.png)


### üîπ –ù–µ–≥–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
![negative example](https://github.com/icelevsha/BERT_vs_Naive_Bayes/blob/main/images/positive.png)

## üîú –ü–ª–∞–Ω—ã –Ω–∞ –±—É–¥—É—â–µ–µ
- –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö.
- –£–ª—É—á—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
